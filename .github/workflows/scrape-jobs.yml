name: Daily Job Scraper

on:
  schedule:
    # Run at 9 AM Bangladesh time (3 AM UTC)
    - cron: '0 3 * * *'
  workflow_dispatch: # Allow manual trigger

env:
  DATABASE_URL: ${{ secrets.DATABASE_URL }}
  VAPID_PUBLIC_KEY: ${{ secrets.VAPID_PUBLIC_KEY }}
  VAPID_PRIVATE_KEY: ${{ secrets.VAPID_PRIVATE_KEY }}
  VAPID_SUBJECT: ${{ secrets.VAPID_SUBJECT }}
  RESEND_API_KEY: ${{ secrets.RESEND_API_KEY }}
  ADMIN_API_KEY: ${{ secrets.ADMIN_API_KEY }}
  RENDER_API_URL: ${{ secrets.RENDER_API_URL }}

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          cd backend
          pip install -r requirements.txt

      - name: Run scraper
        run: |
          cd backend
          python run_scraper.py

      - name: Trigger daily digest notifications
        if: success()
        run: |
          echo "Scraping completed. Triggering daily digest notifications..."
          RESPONSE=$(curl -s -w "\n%{http_code}" -X POST \
            "${RENDER_API_URL}/api/notifications/send-daily-digest" \
            -H "Content-Type: application/json" \
            -H "X-API-Key: ${ADMIN_API_KEY}")
          HTTP_CODE=$(echo "$RESPONSE" | tail -1)
          BODY=$(echo "$RESPONSE" | head -n -1)
          echo "Response ($HTTP_CODE): $BODY"
          if [ "$HTTP_CODE" -ge 200 ] && [ "$HTTP_CODE" -lt 300 ]; then
            echo "Daily digest sent successfully!"
          else
            echo "Warning: Daily digest failed (HTTP $HTTP_CODE), but scraping was successful."
          fi

  notify-on-failure:
    needs: scrape
    if: failure()
    runs-on: ubuntu-latest
    steps:
      - name: Notify failure
        run: |
          echo "Job scraping failed! Check the logs."
